{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crawling 10K data from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#This code is used for downloading 10K report, not 10Q\n",
    "import os, csv, urllib2, time, re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##### Section 1: Extract the\tURLs\tfrom each\tfirms'\tsearch\tresults\treturned by\tEdgar\n",
    "os.chdir('H:\\python\\project')\n",
    "companyListFile = \"CompanyTickerList.csv\"  # a csv file with the list of company ticker symbols and names\n",
    "IndexLinksFile = \"IndexLinks10K.csv\"  # a csv file (output of the current script) with the list of index links for each firm\n",
    "\n",
    "def getIndexLink(companyListFile, FormType):\n",
    "    csvFile = open(companyListFile, \"r\")\n",
    "    csvReader = csv.reader(csvFile, delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "\n",
    "    Full_List = []\n",
    "    for rowData in csvData[1:]:\n",
    "        tickerCode = rowData[0]\n",
    "\n",
    "        urlLink = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=\" + tickerCode + \"&type=\" + FormType + \"&dateb=&owner=exclude&count=100\"\n",
    "        pageRequest = urllib2.Request(urlLink)\n",
    "        pageOpen = urllib2.urlopen(pageRequest)\n",
    "        pageRead = pageOpen.read()\n",
    "\n",
    "        soup = BeautifulSoup(pageRead, \"html.parser\")\n",
    "\n",
    "        # Check if there is a table to extract / code exists in edgar database\n",
    "        try:\n",
    "            table = soup.find(\"table\", {\"class\": \"tableFile2\"})\n",
    "        except:\n",
    "            print \"No tables found or no matching ticker symbol for ticker symbol for\" + tickerCode\n",
    "            return -1\n",
    "\n",
    "        docIndex = 1\n",
    "        for row in table.findAll(\"tr\"):\n",
    "            cells = row.findAll(\"td\")\n",
    "            if len(cells) == 5:\n",
    "                if cells[0].text.strip() == FormType:\n",
    "                    link = cells[1].find(\"a\", {\"id\": \"documentsbutton\"})\n",
    "                    docLink = \"https://www.sec.gov\" + link['href']\n",
    "                    description = cells[2].text.encode(\n",
    "                        'utf8').strip()\n",
    "                    filingDate = cells[3].text.encode('utf8').strip()\n",
    "                    newfilingDate = filingDate.replace(\"-\", \"_\")  ### <=== Change date format from 2012-1-1 to 2012_1_1 so it can be used as part of 10-K file names\n",
    "                    docIndex = docIndex + 1\n",
    "                    rows = [tickerCode, docIndex, docLink, description, filingDate, newfilingDate]\n",
    "                    Full_List.append(rows)\n",
    "\n",
    "    headers = ['Ticker', 'DocIndex', 'IndexLink', 'Description', 'FilingDate', 'NewFilingDate']\n",
    "    data_links = pd.DataFrame(Full_List, columns=headers)\n",
    "    data_links.to_csv(IndexLinksFile, index=False)\n",
    "\n",
    "FormType = \"10-K\" ### <=== Change to other type if needed. For now, we extract \"10-K\" report\n",
    "\n",
    "csvOutput = open(IndexLinksFile, \"a+b\")\n",
    "csvOutput.truncate()\n",
    "\n",
    "getIndexLink(companyListFile, FormType)\n",
    "\n",
    "print \"done!\"\n",
    "\n",
    "#### Section2:\tExtracts the\tURLs\tfor\teach\tfirm's 10-K\treports\n",
    "\n",
    "Form10qListFile = \"List10K.csv\"  # a csv file (output of the current script) with the list of 10-K links for each firm\n",
    "\n",
    "def get10qLink(IndexLinksFile, FormType):\n",
    "    csvFile = open(IndexLinksFile, \"r\")\n",
    "    csvReader = csv.reader(csvFile, delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "\n",
    "    Full_List = []\n",
    "    i = 1\n",
    "    for rowData in csvData[1:]:\n",
    "        Ticker = rowData[0]\n",
    "        DocIndex = rowData[1]\n",
    "        DocLink = rowData[2]\n",
    "        Description = rowData[3]\n",
    "        FileDate = rowData[4]\n",
    "        NewFileDate = rowData[5]\n",
    "\n",
    "        pageRequest = urllib2.Request(DocLink)\n",
    "        pageOpen = urllib2.urlopen(pageRequest)\n",
    "        pageRead = pageOpen.read()\n",
    "\n",
    "        soup = BeautifulSoup(pageRead, \"html.parser\")\n",
    "\n",
    "        # Check if there is a table to extract / code exists in edgar database\n",
    "        try:\n",
    "            table = soup.find(\"table\", {\"summary\": \"Document Format Files\"})\n",
    "        except:\n",
    "            print \"No tables found for link \" + DocLink\n",
    "\n",
    "        for row in table.findAll(\"tr\"):\n",
    "            cells = row.findAll(\"td\")\n",
    "            if len(cells) == 5:\n",
    "                if cells[3].text.strip() == FormType:\n",
    "                    link = cells[2].find(\"a\")\n",
    "                    FormLink= \"https://www.sec.gov\" + link['href']\n",
    "                    FormName = link.text.encode('utf8').strip()\n",
    "        rows = [Ticker, DocIndex, DocLink, Description, FileDate, NewFileDate, FormLink, FormName]\n",
    "        Full_List.append(rows)\n",
    "\n",
    "        nbDocPause = 10  ### <=== Type your number of documents to download in one batch\n",
    "        nbSecPause = 1  ### <=== Type your pausing time in seconds between each batch\n",
    "        if i % nbDocPause == 0:\n",
    "            print i\n",
    "            print \"Pause for \" + str(nbSecPause) + \" second .... \"\n",
    "            time.sleep(float(nbSecPause))\n",
    "        i = i + 1\n",
    "\n",
    "    headers = ['Ticker', 'DocIndex', 'IndexLink', 'Description', 'FilingDate', 'NewFilingDate', 'Form10KLink', 'Form10KName']\n",
    "    data_links = pd.DataFrame(Full_List, columns=headers)\n",
    "    data_links.to_csv(Form10qListFile, index=False)\n",
    "\n",
    "FormType = \"10-K\" ### <=== Change to other type if needed. For now, we extract \"10-K\" report\n",
    "\n",
    "csvOutput = open(Form10qListFile, \"a+b\")\n",
    "csvOutput.truncate()\n",
    "\n",
    "get10qLink(IndexLinksFile, FormType)\n",
    "print \"done!\"\n",
    "\n",
    "##### Section 3: Downloads the\t10-K\treports\tas\tHTML\tfiles\n",
    "SubPath = \"./10-K report/HTML/\"  # <===The subfolder with the 10-K files in HTML format\n",
    "logFile = \"DownloadLog10K.csv\"  # a csv file (output of the current script) with the download history of 10-K forms\n",
    "\n",
    "def dowmload10q(Form10qListFile, FormYears):\n",
    "    csvFile = open(Form10qListFile, \"r\")\n",
    "    csvReader = csv.reader(csvFile, delimiter=\",\")\n",
    "    csvData = list(csvReader)\n",
    "\n",
    "    Full_List = []\n",
    "    i = 1\n",
    "    for rowData in csvData[1:]:\n",
    "        Ticker = rowData[0]\n",
    "        DocIndex = rowData[1]\n",
    "        IndexLink = rowData[2]\n",
    "        Description = rowData[3]\n",
    "        FilingDate = rowData[4]\n",
    "        NewFilingDate = rowData[5]\n",
    "        FormLink = rowData[6]\n",
    "        FormName = rowData[7]\n",
    "\n",
    "        for year in FormYears:\n",
    "            if year in FilingDate:\n",
    "                pageRequest = urllib2.Request(FormLink)\n",
    "                pageOpen = urllib2.urlopen(pageRequest)\n",
    "                pageRead = pageOpen.read()\n",
    "\n",
    "                if \".htm\" in FormName:\n",
    "                    try:\n",
    "                        htmlname = Ticker + \"_\" + DocIndex + \"_\" + NewFilingDate + \".htm\"\n",
    "                        htmlpath = SubPath + htmlname\n",
    "                        htmlfile = open(htmlpath, 'wb')\n",
    "                        htmlfile.write(pageRead)\n",
    "                        htmlfile.close()\n",
    "                        rows = [Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink, FormName, htmlname, \"\"]\n",
    "                    except:\n",
    "                        rows = [Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink, FormName, \"not downloaded\"]\n",
    "                elif \".txt\" in FormName:\n",
    "                    try:\n",
    "                        textname = Ticker + \"_\" + DocIndex + \"_\" + NewFilingDate + \".txt\"\n",
    "                        textpath = SubPath + textname\n",
    "                        textfile = open(textpath, 'wb')\n",
    "                        textfile.write(pageRead)\n",
    "                        textfile.close()\n",
    "                        rows = [Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink, FormName, textname, \"\"]\n",
    "                    except:\n",
    "                        rows = [Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink, FormName, \"not downloaded\"]\n",
    "                else:\n",
    "                    rows = [Ticker, DocIndex, IndexLink, Description, FilingDate, NewFilingDate, FormLink, FormName, \"\", \"No form\"]\n",
    "                Full_List.append(rows)\n",
    "\n",
    "        nbDocPause = 10  ### <=== Type your number of documents to download in one batch\n",
    "        nbSecPause = 1  ### <=== Type your pausing time in seconds between each batch\n",
    "        if i % nbDocPause == 0:\n",
    "            print i\n",
    "            print \"Pause for \" + str(nbSecPause) + \" second .... \"\n",
    "            time.sleep(float(nbSecPause))\n",
    "        i = i + 1\n",
    "    headers = ['Ticker', 'DocIndex', 'IndexLink', 'Description', 'FilingDate', 'NewFilingDate', 'Form10KLink', 'Form10KName', \"FileName\", \"Note\"]\n",
    "    data_links = pd.DataFrame(Full_List, columns=headers)\n",
    "    data_links.to_csv(logFile, index=False)\n",
    "\n",
    "if not os.path.isdir(SubPath):\n",
    "    os.makedirs(SubPath)\n",
    "\n",
    "FormYears = ['2006','2007','2008','2009','2010','2011', '2012', '2013', '2014', '2015', '2016']  ### <=== Type the years of documents you wish to download here, now we only\n",
    "                                                ### download files for all these years listed.\n",
    "\n",
    "csvOutput = open(logFile, \"a+b\")\n",
    "csvOutput.truncate()\n",
    "\n",
    "dowmload10q(Form10qListFile, FormYears)\n",
    "print \"done!\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0330214047542 0.0254920942239 ko2016\n",
      "0.0311302681992 0.0249042145594 ko2015\n",
      "0.0309186150505 0.0241471828287 ko2014\n",
      "0.0295445219532 0.024620434961 ko2013\n",
      "0.0310969961625 0.0227603546381 ko2012\n",
      "0.029887029887 0.023452023452 ko2011\n",
      "0.0262192885955 0.0243857719105 ko2010\n",
      "0.0262192885955 0.0243857719105 ko2009\n",
      "0.0255748979153 0.020202020202 ko2008\n",
      "0.0260481270156 0.023319275614 ko2007\n",
      "0.0682401231401 0.0218060543869 ko2006\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#小组分工\n",
    "from __future__ import division\n",
    "from string import punctuation\n",
    "import csv\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "os.chdir('C:/Users/bjd/git/Finance/python spring/project/textfile')#改成你们自己的路径\n",
    "#请打开你选的股票的一个年报，ctrl+f启用搜索，迅速定位到risk factors，将risk factors\n",
    "#这个部分整个复制下来，保存到一个新的txt文档中，并将之命名。记住这个txt文档，应该在\n",
    "#上面那行的路径里面创建\n",
    "urllib.urlretrieve('http://www.unc.edu/~ncaren/haphazard/positive.txt', 'positive.txt')\n",
    "urllib.urlretrieve('http://www.unc.edu/~ncaren/haphazard/negative.txt', 'negative.txt')\n",
    "\n",
    "\n",
    "#把下面这个text的内容改成你们的txt文件的名字\n",
    "text='ko2016','ko2015','ko2014','ko2013','ko2012','ko2011','ko2010','ko2009','ko2008','ko2007','ko2006'\n",
    "for i in range(len(text)):\n",
    "    msgs=open(text[i]+'.txt').read().lower()\n",
    "    pos = open(\"positive.txt\").read()\n",
    "    positive_words = pos.split('\\n')\n",
    "    positive_number = 0\n",
    "\n",
    "    neg = open('negative.txt').read()\n",
    "    negative_words = neg.split('\\n')\n",
    "    negative_number = 0\n",
    "\n",
    "    for p in punctuation:\n",
    "        \n",
    "        words = msgs.replace(p, '') #把标点符号去掉，换成空格，此时words是一个string，\n",
    "                                #里面的元素是一个个字母\n",
    "        words = list(words.split(' '))#以空格为间隔，将words变成list，这样里面的元素\n",
    "                              #就是一个个单词了\n",
    "        word_count = len(words)\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "           positive_number += 1\n",
    "        elif word in negative_words:\n",
    "           negative_number += 1\n",
    "    #print word_count,positive_number, negative_number, text[i]\n",
    "### Uncomment if you need percentage\n",
    "    positive_pencentage = positive_number / word_count \n",
    "    negative_percentage = negative_number / word_count\n",
    "    print positive_pencentage, negative_percentage, text[i]\n",
    "\n",
    "\n",
    "#将这次运行得到的数值，保存进一个csv文件中。这里直接参照老师上传的10—K1A来（把老师\n",
    "#那个文件里的数据全部删掉，只保留第一行，然后填进去，filename可以只填对应年份"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_datareader.data as pdr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime, os, csv\n",
    "import statsmodels.api as sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "os.chdir( \"C:/Users/bjd/git/Finance/python spring/project\")\n",
    "\n",
    "\n",
    "\n",
    "input_path10K1A = \"10-K1A.csv\"\n",
    "csvFile10K1A = open(input_path10K1A, \"r\")\n",
    "csvReader10K1A = csv.reader(csvFile10K1A, delimiter=\",\")\n",
    "csvData10K1A = list(csvReader10K1A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path = \"CompanyTickerList.csv\"\n",
    "csvFile = open(input_path, \"r\")\n",
    "csvReader = csv.reader(csvFile, delimiter=\",\")\n",
    "csvData = list(csvReader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2006, 1, 1) #<==change start time here\n",
    "end = datetime.datetime(2017, 1, 1) #<==change end time here\n",
    "\n",
    "# Read-in data for SPY\n",
    "spy = pdr.DataReader(\"SPY\", 'yahoo', start, end)\n",
    "spy_adj = spy['Adj Close']\n",
    "spy_ret = np.log(spy_adj).diff().dropna()\n",
    "\n",
    "## Read-in data for stocks\n",
    "stocks = []\n",
    "for rowData in csvData[1:]:\n",
    "    ticker = rowData[0]\n",
    "    stocks.append(ticker)\n",
    "price = pdr.DataReader(stocks, 'yahoo', start, end)\n",
    "key_word = 'Adj Close'\n",
    "cleanData = price.ix[key_word]\n",
    "adj_close = pd.DataFrame(cleanData)\n",
    "ret = np.log(adj_close).diff().dropna()\n",
    "rows = ret.shape[0]\n",
    "\n",
    "headers = sorted(stocks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1): # <===N=20 securities in this portfolio. Change this to fit the size of your portfolio\n",
    "    stock = ret.ix[:, i]\n",
    "    stock_name = headers[i]\n",
    "    spy_ret = sm.add_constant(spy_ret)\n",
    "    model = sm.OLS(stock, spy_ret).fit()\n",
    "    ret[stock_name] = model.resid #<===dataframe of residual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CAR   R-squared:                       0.087\n",
      "Model:                            OLS   Adj. R-squared:                  0.063\n",
      "Method:                 Least Squares   F-statistic:                     3.544\n",
      "Date:                Mon, 01 May 2017   Prob (F-statistic):             0.0339\n",
      "Time:                        15:20:32   Log-Likelihood:                 207.07\n",
      "No. Observations:                  77   AIC:                            -408.1\n",
      "Df Residuals:                      74   BIC:                            -401.1\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "--------------------------------------------------------------------------------\n",
      "const            0.0820      0.034      2.398      0.019         0.014     0.150\n",
      "positive_pct    -0.0091      0.172     -0.053      0.958        -0.352     0.334\n",
      "negative_pct    -3.5052      1.364     -2.571      0.012        -6.222    -0.788\n",
      "==============================================================================\n",
      "Omnibus:                        7.051   Durbin-Watson:                   0.478\n",
      "Prob(Omnibus):                  0.029   Jarque-Bera (JB):                6.447\n",
      "Skew:                          -0.679   Prob(JB):                       0.0398\n",
      "Kurtosis:                       3.408   Cond. No.                         715.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "car = []\n",
    "for rowData in csvData10K1A[1:]:\n",
    "    ticker = rowData[0]\n",
    "    filingdate = rowData[2]\n",
    "    positive_pct = rowData[5]\n",
    "    negative_pct = rowData[7]\n",
    "\n",
    "    try:\n",
    "        tic_index = headers.index(ticker)\n",
    "    except ValueError:\n",
    "        print \"ticker not in list.\"\n",
    "    residual = ret.ix[:, tic_index]\n",
    "    date_index = residual.index.get_loc(filingdate)\n",
    "    x = 3 #<==analayze 3 days before and 3 days after the report date.\n",
    "    date_start = date_index - x\n",
    "    for i in range(1, 2+2*x):\n",
    "        row = [sum(residual[date_start: date_start + i]), positive_pct, negative_pct]\n",
    "        car.append(row)\n",
    "\n",
    "df = pd.DataFrame(car, columns=['CAR', 'positive_pct', 'negative_pct'])\n",
    "\n",
    "Y = df['CAR']\n",
    "X = df[['positive_pct', 'negative_pct']]\n",
    "X = sm.add_constant(X)\n",
    "reg = sm.OLS(Y, X.astype(float)).fit()\n",
    "print reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
